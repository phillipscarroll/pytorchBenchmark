{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train And Test Benchmark Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Once To Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Constants\n",
    "TRAIN_GOOD_FILE_COUNT = 250\n",
    "TEST_GOOD_FILE_COUNT = 50\n",
    "TRAIN_BAD_FILE_COUNT = 250\n",
    "TEST_BAD_FILE_COUNT = 50\n",
    "DATA_FOLDER = \"data\"\n",
    "WDSDataset_FOLDER = f\"{DATA_FOLDER}/benchmark_dataset\"\n",
    "\n",
    "# Generate Good files\n",
    "good_files = []\n",
    "for i in range(TRAIN_GOOD_FILE_COUNT + TEST_GOOD_FILE_COUNT):\n",
    "    data = {\n",
    "        \"id\": random.randint(100000, 999999),\n",
    "        \"name\": {\n",
    "            \"first\": f\"John\",\n",
    "            \"last\": f\"Doe {i+1}\"\n",
    "        },\n",
    "        \"age\": random.randint(18, 60),\n",
    "        \"score\": {\n",
    "            \"math\": round(random.uniform(0.6, 1.0), 2),\n",
    "            \"science\": round(random.uniform(0.6, 1.0), 2),\n",
    "            \"english\": round(random.uniform(0.6, 1.0), 2)\n",
    "        },\n",
    "        \"address\": {\n",
    "            \"street\": f\"{random.randint(100, 999)} Main St\",\n",
    "            \"city\": \"Anytown\",\n",
    "            \"state\": \"CA\",\n",
    "            \"zip\": f\"{random.randint(10000, 99999)}\"\n",
    "        },\n",
    "        \"contacts\": [\n",
    "            {\"type\": \"email\", \"value\": f\"john.doe{i+1}@example.com\"},\n",
    "            {\"type\": \"phone\", \"value\": f\"555-{random.randint(1000, 9999)}\"}\n",
    "        ]\n",
    "    }\n",
    "    good_files.append(json.dumps(data))\n",
    "\n",
    "# Generate Bad files\n",
    "bad_files = []\n",
    "for i in range(TRAIN_BAD_FILE_COUNT + TEST_BAD_FILE_COUNT):\n",
    "    data = {\n",
    "        \"id\": random.randint(100000, 999999),\n",
    "        \"name\": {\n",
    "            \"first\": f\"John\",\n",
    "            \"last\": f\"Doe {i+1}\"\n",
    "        },\n",
    "        \"age\": random.randint(18, 60),\n",
    "        \"score\": {\n",
    "            \"math\": round(random.uniform(0.4, 0.9), 2),\n",
    "            \"science\": round(random.uniform(0.4, 0.9), 2),\n",
    "            \"english\": round(random.uniform(0.4, 0.9), 2)\n",
    "        },\n",
    "        \"address\": {\n",
    "            \"street\": f\"{random.randint(100, 999)} Main St\",\n",
    "            \"city\": \"Anytown\",\n",
    "            \"state\": \"CA\",\n",
    "            \"zip\": f\"{random.randint(10000, 99999)}\"\n",
    "        },\n",
    "        \"contacts\": [\n",
    "            {\"type\": \"email\", \"value\": f\"john.doe{i+1}@example.com\"},\n",
    "            {\"type\": \"phone\", \"value\": f\"555-{random.randint(1000, 9999)}\"}\n",
    "        ],\n",
    "        \"feedback\": [\"bad\", \"not good\", \"block\"][random.randint(0, 2)]\n",
    "    }\n",
    "    bad_files.append(json.dumps(data))\n",
    "\n",
    "# Create train and test folder structure\n",
    "train_good_folder_path = f\"{WDSDataset_FOLDER}/train/good\"\n",
    "train_bad_folder_path = f\"{WDSDataset_FOLDER}/train/bad\"\n",
    "test_good_folder_path = f\"{WDSDataset_FOLDER}/test/good\"\n",
    "test_bad_folder_path = f\"{WDSDataset_FOLDER}/test/bad\"\n",
    "\n",
    "os.makedirs(train_good_folder_path, exist_ok=True)\n",
    "os.makedirs(train_bad_folder_path, exist_ok=True)\n",
    "os.makedirs(test_good_folder_path, exist_ok=True)\n",
    "os.makedirs(test_bad_folder_path, exist_ok=True)\n",
    "\n",
    "# Save Good files to train/GOOD and test/GOOD folders\n",
    "for i in range(TRAIN_GOOD_FILE_COUNT):\n",
    "    good_file_path = f\"{train_good_folder_path}/good{i+1}.json\"\n",
    "    with open(good_file_path, \"w\") as f:\n",
    "        f.write(good_files[i])\n",
    "\n",
    "for i in range(TEST_GOOD_FILE_COUNT):\n",
    "    good_file_path = f\"{test_good_folder_path}/good{i+1}.json\"\n",
    "    with open(good_file_path, \"w\") as f:\n",
    "        f.write(good_files[TRAIN_GOOD_FILE_COUNT + i])\n",
    "\n",
    "# Save Bad files to train/BAD and test/BAD folders\n",
    "for i in range(TRAIN_BAD_FILE_COUNT):\n",
    "    bad_file_path = f\"{train_bad_folder_path}/bad{i+1}.json\"\n",
    "    with open(bad_file_path, \"w\") as f:\n",
    "        f.write(bad_files[i])\n",
    "\n",
    "for i in range(TEST_BAD_FILE_COUNT):\n",
    "    bad_file_path = f\"{test_bad_folder_path}/bad{i+1}.json\"\n",
    "    with open(bad_file_path, \"w\") as f:\n",
    "        f.write(bad_files[TRAIN_BAD_FILE_COUNT + i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch.xpu.amp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertForSequenceClassification, AdamW\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradScaler, autocast\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxpu\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradScaler, autocast\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdamW\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradScaler\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch.xpu.amp'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import AdamW\n",
    "from torch.amp import GradScaler\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "# Function to set the number of CPU threads\n",
    "def set_cpu_thread_cap(num_threads):\n",
    "    torch.set_num_threads(num_threads)\n",
    "    torch.set_num_interop_threads(num_threads)\n",
    "    print(f\"Set max CPU threads to: {num_threads}\")\n",
    "\n",
    "# Set the device type and device ID\n",
    "device_type = \"xpu\" if torch.xpu.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_id = 0  # Change this to the desired device index, e.g., 0 or 1\n",
    "# Combine device type and device ID to create the device variable\n",
    "device = torch.device(f\"{device_type}:{device_id}\" if device_type else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "# If running on CPU, set this manually, otherwise comment out\n",
    "#device = \"cpu\"\n",
    "\n",
    "# If running on CPU, set the number of threads\n",
    "if device == 'cpu':\n",
    "    set_cpu_thread_cap(20)  # Set this to the desired number of CPU threads\n",
    "\n",
    "\n",
    "class JSONDataset(Dataset):\n",
    "    def __init__(self, data_dir, tokenizer, max_length=512):\n",
    "        self.data_dir = data_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.data = []\n",
    "        \n",
    "        # Load and preprocess data\n",
    "        for label in ['good', 'bad']:\n",
    "            label_dir = os.path.join(data_dir, label)\n",
    "            for file_name in os.listdir(label_dir):\n",
    "                if file_name.endswith('.json'):\n",
    "                    file_path = os.path.join(label_dir, file_name)\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        script_data = json.load(f)\n",
    "                        # Convert JSON data to a string (simple approach, can be improved)\n",
    "                        text = str(script_data)\n",
    "                        \n",
    "                        self.data.append((text, label))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text, label = self.data[idx]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        # Convert to tensors\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(1 if label == 'good' else 0, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        return item\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Move the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dir = 'data\\\\benchmark_dataset\\\\train'\n",
    "test_dir = 'data\\\\benchmark_dataset\\\\test'\n",
    "\n",
    "train_dataset = JSONDataset(train_dir, tokenizer)\n",
    "test_dataset = JSONDataset(test_dir, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Initialize GradScaler for mixed precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, train_loader, optimizer, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Testing function\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, target_names=['bad', 'good'])\n",
    "    \n",
    "    return accuracy, report\n",
    "\n",
    "# Training loop\n",
    "epochs = 1\n",
    "\n",
    "benchmarkLoops = 10\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the training loop with tqdm for progress bar\n",
    "for i in tqdm.tqdm(range(benchmarkLoops)):\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, scaler)\n",
    "        accuracy, report = test_model(model, test_loader)\n",
    "        # print time and loop number\n",
    "        print(f\"Loop {i+1} - Epoch {epoch+1} - Train Loss: {train_loss:.4f} - Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# End timer\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Batch Size 5, 1 Epoch, 10 Iterations Mixed Precision\n",
    "\n",
    "* ARC A770 16GB (No AMP)\n",
    "  * Elapsed time: 229.07 seconds\n",
    "* RTX 4070 Super 12GB\n",
    "  * Elapsed time: 78.27 seconds\n",
    "* Titan Xp 12GB\n",
    "  * Elapsed time: 375.25 seconds\n",
    "* i9-14900K 5.4ghz all core locked 64GB 6800MHZ DDR5 20 threads\n",
    "  * Elapsed time: 2381.41 seconds\n",
    "* Ultra 155H ARC 16GB Total with 8GB Shared RAM (Laptop) (No AMP)\n",
    "  * Elapsed time: 1457.17 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
